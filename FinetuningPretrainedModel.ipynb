{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaa-04/Finetuning_Pretrained_Model_of_HuggingFace/blob/main/FinetuningPretrainedModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6s6AcPt08K5"
      },
      "outputs": [],
      "source": [
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install bitsandBytes\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v16t51CO_NR6"
      },
      "outputs": [],
      "source": [
        "!pip install GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZv4rCgtAFuQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import GPUtil\n",
        "import os\n",
        "\n",
        "GPUtil.showUtilization()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available, using CPU instead\")\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub5Kq-xbAyqo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84nZqOxhBb8C"
      },
      "outputs": [],
      "source": [
        "if \"COLAB_GPU\" in os.environ:\n",
        "  !huggingface-cli login\n",
        "else:\n",
        "  notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AytCd6RuBj5u"
      },
      "outputs": [],
      "source": [
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu8kYhEgB61m"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"AlignmentLab-AI/agentcode\")\n",
        "\n",
        "# Display the first few examples of the training split\n",
        "# display(dataset['train'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W28Ho847E53x"
      },
      "outputs": [],
      "source": [
        "display(dataset['train'][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_c3rMnbEfYJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T941FWBIG8DB"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = []\n",
        "for phrase in dataset['train']:\n",
        "  tokenized_train_dataset.append(tokenizer(phrase[\"INSTRUCTION\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqXip5GPHWQ7"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFJzKBcVHX4N"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y1SDgStHpOi"
      },
      "outputs": [],
      "source": [
        "tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmW4KgZlHucb"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02gX--VxILA3"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=\"./finetunedModel\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=1e-4,\n",
        "        max_steps=20,\n",
        "        bf16=False,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_dir=\"./log\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_steps=50,\n",
        "        logging_steps=10\n",
        "\n",
        "),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache=False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMOLdfgmKQ02"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, LlamaTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "nf4Config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=nf4Config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOIhR2C2MISD"
      },
      "outputs": [],
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True\n",
        "\n",
        "                              )\n",
        "\n",
        "modelFinetuned = PeftModel.from_pretrained(base_model, \"finetunedModel/checkpoint-20\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsl5SpQrMblq"
      },
      "outputs": [],
      "source": [
        "user_question = \"Please provide a brief explanation of how to create and handle PDF files with PyPDF2 in Python, including its capabilities and limitations.\"\n",
        "\n",
        "eval_prompt = f\"Question: {user_question} Just answer this question accurately and concisely.\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens=1024)[0], skip_special_tokens=True))\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSP6lR1YNWnJ"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "# Define a function that uses the finetuned model to generate text\n",
        "def generate_response(user_input):\n",
        "    eval_prompt = f\"Question: {user_input} Just answer this question accurately and concisely.\\n\"\n",
        "    promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    modelFinetuned.eval()\n",
        "    with torch.no_grad():\n",
        "        output_tokens = modelFinetuned.generate(**promptTokenized, max_new_tokens=1024)\n",
        "        response = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    # Clean up the response to remove the original prompt\n",
        "    # Find the index where the model's generated text starts\n",
        "    prompt_end_index = response.find(\"Just answer this question accurately and concisely.\\n\")\n",
        "    if prompt_end_index != -1:\n",
        "        # Add the length of the prompt part we want to keep\n",
        "        response_start_index = prompt_end_index + len(\"Just answer this question accurately and concisely.\\n\")\n",
        "        # Find the start of the model's actual response after the prompt\n",
        "        response_start = response[response_start_index:].strip()\n",
        "        # Assuming the model output starts with \":\" as seen in previous runs\n",
        "        if response_start.startswith(\":\"):\n",
        "          response = response_start[1:].strip() # Remove the leading \":\" and any extra whitespace\n",
        "        else:\n",
        "          response = response_start # Keep the response as is if it doesn't start with \":\"\n",
        "    else:\n",
        "        # If the prompt structure isn't found, return the full response\n",
        "        response = response.strip()\n",
        "\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_response,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Finetuned Llama-2 Chatbot\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}